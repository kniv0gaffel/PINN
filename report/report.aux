\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\citation{*}
\@writefile{toc}{\contentsline {section}{Introduction}{3}{chapter*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Wave Equation}{4}{section.0.1}\protected@file@percent }
\newlabel{sec:wave}{{1}{4}{Introduction}{section.0.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Data}{4}{section.0.2}\protected@file@percent }
\newlabel{sec:data}{{2}{4}{Introduction}{section.0.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Unsupervised Data}{4}{equation.0.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Supervised Data}{5}{equation.0.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Sequential Supervised Data (RNN)}{5}{equation.0.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{5}{section.0.3}\protected@file@percent }
\newlabel{sec:methods}{{3}{5}{Introduction}{section.0.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Finite Difference Scheme}{5}{subsection.0.3.1}\protected@file@percent }
\newlabel{sec:finite}{{3.1}{5}{Introduction}{subsection.0.3.1}{}}
\citation{fys-stk}
\citation{krishnapriyan2021characterizing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Physics Informed Neural Networks (PINNs)}{6}{subsection.0.3.2}\protected@file@percent }
\newlabel{sec:DNN}{{3.2}{6}{Introduction}{subsection.0.3.2}{}}
\citation{krishnapriyan2021characterizing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Hybrid Solver}{7}{subsection.0.3.3}\protected@file@percent }
\newlabel{sec:hybrid}{{3.3}{7}{Introduction}{subsection.0.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Recurrent Neural Networks (RNNs)}{7}{subsection.0.3.4}\protected@file@percent }
\newlabel{sec:rnn}{{3.4}{7}{Introduction}{subsection.0.3.4}{}}
\citation{lstm}
\citation{hu2022neuralpde}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results and Analysis}{8}{section.0.4}\protected@file@percent }
\newlabel{sec:resultsdiscussion}{{4}{8}{Introduction}{section.0.4}{}}
\newlabel{tab:toyscores}{{1}{8}{Introduction}{table.0.1}{}}
\@writefile{lot}{\contentsline {tcolorbox}{\numberline {1}{\ignorespaces Mean squared error between analytical solution and different solvers. Lower is better.}}{8}{table.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Potential of PINNs:}{8}{tcbfloat.caption.3}\protected@file@percent }
\citation{tensorflow2015-whitepaper}
\@writefile{toc}{\contentsline {subsubsection}{Overfitting Considerations in Different Solvers:}{9}{tcbfloat.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Data Normalization and Scale Considerations:}{9}{tcbfloat.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Potential and Limitations of Neural Networks for PDEs:}{9}{tcbfloat.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Analytical solution to the wave equation. \relax }}{9}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:wave_finite}{{1}{9}{Analytical solution to the wave equation. \relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Finite difference solution to the wave equation. \relax }}{9}{figure.caption.4}\protected@file@percent }
\newlabel{fig:wave_analytic}{{2}{9}{Finite difference solution to the wave equation. \relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  PINN solution to the wave equation. \relax }}{10}{figure.caption.5}\protected@file@percent }
\newlabel{fig:wave_own_dnn}{{3}{10}{PINN solution to the wave equation. \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  The loss curve for the PINN. The loss is very noisy, but it seems to not have converged after 50000 epochs. \relax }}{10}{figure.caption.5}\protected@file@percent }
\newlabel{fig:wave_tf_dnn}{{4}{10}{The loss curve for the PINN. The loss is very noisy, but it seems to not have converged after 50000 epochs. \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  The hybrid solver's solution to the wave equation. \relax }}{10}{figure.caption.6}\protected@file@percent }
\newlabel{fig:wave_own_dnn}{{5}{10}{The hybrid solver's solution to the wave equation. \relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  The loss curve for the hybrid solver. The loss is less noisy than the PINN, with a clear downward trend. \relax }}{10}{figure.caption.6}\protected@file@percent }
\newlabel{fig:wave_tf_dnn}{{6}{10}{The loss curve for the hybrid solver. The loss is less noisy than the PINN, with a clear downward trend. \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Hyperparameters and Architecture}{10}{subsection.0.4.1}\protected@file@percent }
\newlabel{sec:hyperparameters}{{4.1}{10}{Introduction}{subsection.0.4.1}{}}
\citation{krishnapriyan2021characterizing}
\citation{hu2022neuralpde}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  RNN solution to the wave equation on the same time frame as the training data. The first 10 points are not shown as they are used as input to the RNN. \relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{fig:wave_own_dnn}{{7}{11}{RNN solution to the wave equation on the same time frame as the training data. The first 10 points are not shown as they are used as input to the RNN. \relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  The RNN predicts the evolution of the wave after the initial time frame of the training data. We can see that the accuracy diminishes quickly. \relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{fig:wave_tf_dnn}{{8}{11}{The RNN predicts the evolution of the wave after the initial time frame of the training data. We can see that the accuracy diminishes quickly. \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Comparisons}{11}{subsection.0.4.2}\protected@file@percent }
\newlabel{sec:comparisons}{{4.2}{11}{Introduction}{subsection.0.4.2}{}}
\bibdata{report}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{12}{section.0.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{12}{Introduction}{section.0.5}{}}
\bibcite{tensorflow2015-whitepaper}{1}
\bibcite{fys-stk}{2}
\bibcite{lstm}{3}
\bibcite{hu2022neuralpde}{4}
\bibcite{krishnapriyan2021characterizing}{5}
\bibcite{gpt}{6}
\bibcite{MachineLearningProjects_2023}{7}
\bibstyle{plain}
\@writefile{toc}{\contentsline {section}{Bibliography}{13}{chapter*.8}\protected@file@percent }
\gdef \@abspage@last{13}
